{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"congitveAndPsychologicalTest/","title":"Cogntive and Psychological tests","text":""},{"location":"congitveAndPsychologicalTest/#nasa-task-load-index-tlx","title":"NASA Task Load Index (TLX)","text":""},{"location":"congitveAndPsychologicalTest/#aptitude","title":"Aptitude","text":"<p>The LLAMA aptitude test (Meara, 2005)</p>"},{"location":"corpus/","title":"Corpus","text":""},{"location":"corpus/#resources","title":"Resources","text":""},{"location":"corpus/#antconc","title":"AntConc","text":""},{"location":"corpus/#sketch-engine","title":"Sketch Engine","text":""},{"location":"corpus/#coh-metrix","title":"Coh-Metrix","text":"<p>Coh-Metrix is a computational tool used for analyzing and measuring the linguistic features of written text. It was developed by Danielle McNamara and her colleagues at the University of Memphis, and it is designed to provide insights into the complexity and readability of written text.</p> <p>Coh-Metrix uses a variety of linguistic features, such as word frequency, sentence length, and syntactic complexity, to analyze the text. It also takes into account the discourse features of the text, such as the coherence and cohesion of the ideas presented.</p> <p>Coh-Metrix can be used for a variety of purposes, such as assessing the readability of text, identifying areas of difficulty for readers, and evaluating the effectiveness of instructional materials. It can also be used to compare different types of texts, such as academic articles, news articles, and fiction.</p> <p>The tool provides a range of measures, such as the Flesch-Kincaid Grade Level, which estimates the grade level of the text, and the Cohesion Index, which measures the coherence and cohesion of the text. Coh-Metrix also provides visualizations of the text, such as word clouds and network diagrams, to help users understand the structure and content of the text.</p>"},{"location":"corpus/#textevaluator","title":"TextEvaluator","text":"<p>TextEvaluator is a software tool designed to evaluate the readability and complexity of written text. It is intended for use by educators, writers, and publishers to assess the readability of text and to improve the quality of written communication.</p> <p>TextEvaluator uses a variety of linguistic features, such as sentence length, word frequency, and reading level, to analyze the text. It also takes into account the discourse features of the text, such as coherence and cohesion, to provide a comprehensive evaluation of the text's readability.</p> <p>The tool provides a range of measures, such as the Flesch-Kincaid Grade Level, which estimates the grade level of the text, and the Gunning Fog Index, which measures the complexity of the text. TextEvaluator also provides suggestions for improving the readability of the text, such as simplifying sentence structure and using more common words.</p> <p>TextEvaluator is available as a web-based tool that can be accessed through a browser. It is easy to use, and the results can be downloaded as a report or shared with others through a link.</p> <p>TextEvaluator (2023). TextEvaluator tool (version 5.1). Educational Testing Service.</p>"},{"location":"corpus/#lexical-complexity-analyzer-lca","title":"Lexical Complexity Analyzer (LCA)","text":"<p>The Lexical Complexity Analyzer (LCA) is a software tool designed to analyze the lexical complexity of written text. It is intended for use by researchers, educators, and writers to assess the complexity of vocabulary used in written text and to improve the quality of written communication.</p> <p>The LCA uses a variety of linguistic features, such as word frequency, word length, and word concreteness, to analyze the lexical complexity of the text. It also takes into account the discourse features of the text, such as coherence and cohesion, to provide a comprehensive evaluation of the text's lexical complexity.</p> <p>The tool provides a range of measures, such as the Type-Token Ratio, which measures the diversity of vocabulary used in the text, and the Word Information Content, which measures the informativeness of the words used in the text. The LCA also provides suggestions for improving the lexical complexity of the text, such as using more concrete words and avoiding jargon.</p> <p>The LCA is available as a web-based tool that can be accessed through a browser. It is easy to use, and the results can be downloaded as a report or shared with others through a link.</p> <p>https://sites.psu.edu/xxl13/lca/</p>"},{"location":"corpus/#l2-syntactical-complexity-analyzer-l2sca","title":"L2 Syntactical Complexity Analyzer (L2SCA)","text":""},{"location":"corpus/#stanford-log-linear-part-of-speech-tagger","title":"Stanford Log-linear Part-Of-Speech Tagger","text":"<p>The Stanford Log-linear Part-Of-Speech (POS) Tagger is a natural language processing tool used to identify the part of speech (e.g., noun, verb, adjective) of each word in a text. The tagger is based on a statistical model that uses machine learning algorithms to assign the most probable POS tag to each word in a sentence.</p> <p>The Stanford POS Tagger uses a log-linear model that combines several different features of the text to predict the correct POS tag for each word. These features include the word itself, the context in which it appears, and the surrounding words. The model is trained on a large corpus of text, and the accuracy of the tagger is continually improved through iterative training.</p> <p>The Stanford POS Tagger is widely used in natural language processing applications, such as text analysis, machine translation, and information retrieval. The tagger is available as a software package that can be downloaded and used for free, and it can be integrated into other applications through an API.</p>"},{"location":"corpus/#kfngram","title":"KfNgram","text":""},{"location":"corpus/#taassc","title":"TAASSC","text":""},{"location":"corpus/#wmatrix","title":"Wmatrix","text":""},{"location":"corpus/#wordsmith","title":"Wordsmith","text":""},{"location":"corpus/#arte","title":"ARTE","text":""},{"location":"corpus/#cla","title":"CLA","text":""},{"location":"corpus/#crat","title":"CRAT","text":""},{"location":"corpus/#gamet","title":"GAMET","text":""},{"location":"corpus/#seance","title":"SEANCE","text":""},{"location":"corpus/#sinlp","title":"SiNLP","text":""},{"location":"corpus/#taaco","title":"TAACO","text":""},{"location":"corpus/#taaled","title":"TAALED","text":""},{"location":"corpus/#taales","title":"TAALES","text":""},{"location":"corpus/#tammi","title":"TAMMI","text":""},{"location":"grammar/","title":"Grammar","text":""},{"location":"grammar/#timed-and-untimed-grammaticality-judgment-test","title":"Timed and Untimed grammaticality judgment test","text":"<p>The Timed and Untimed Grammaticality Judgment Test is a language assessment tool used to evaluate an individual's ability to recognize and judge the grammaticality of sentences. The test is designed to assess the individual's knowledge of syntax and their ability to apply it in a variety of contexts.</p> <p>The test consists of a series of sentences presented to the test taker, who is asked to judge whether each sentence is grammatically correct or incorrect. The test can be administered in a timed or untimed format, depending on the purpose of the assessment and the needs of the test taker.</p> <p>In the timed format, the test taker is given a set amount of time to complete the test, typically between 15 and 30 minutes. The timed format is designed to assess the individual's ability to make quick and accurate judgments about sentence grammaticality.</p> <p>In the untimed format, the test taker is not restricted by time and can take as long as necessary to complete the test. The untimed format is designed to assess the individual's overall knowledge of syntax and their ability to apply it accurately.</p> <p>The Timed and Untimed Grammaticality Judgment Test has been used in various research studies and has been shown to have high reliability and validity. It can be used to assess language proficiency in a variety of contexts, such as second language acquisition and language disorders.</p> <p>(Ellis, 2005)</p>"},{"location":"local_index/","title":"Introduction","text":"<p>Data collection in Second Language Research: A Catalogue of Scales, Instruments, Tools, and Tasks</p> <p>\u201cThe idea is to make a collection of instruments, tools, and tasks used in L2 research. Using the book, SLA researchers can browse different instruments, and know when and how to use them. For each instrument or scale, the book will show a popularity index [score or icon showing how popular the tool is. It is the number of the citations each instrument receives divided by the sum of the citations of all instruments in the same category]\u201d</p> <p>The objectives of using tools and instruments in research for data collection are:</p> <p>To ensure the accuracy and reliability of data: By using standardized tools and instruments, researchers can ensure that data is collected in a consistent and reliable manner, reducing the likelihood of errors or bias in the data.</p> <p>To increase the validity of research findings: Validity refers to the extent to which a study measures what it intends to measure. The use of appropriate tools and instruments can increase the validity of research findings by ensuring that the data collected is relevant to the research questions or hypotheses.</p> <p>To enhance the efficiency of data collection: Tools and instruments can help to streamline the data collection process, making it more efficient and reducing the amount of time and resources required to collect and analyze data.</p> <p>To facilitate comparisons across studies: Standardized tools and instruments allow for comparisons to be made across different studies, making it easier to identify patterns and trends in the data.</p> <p>To increase the generalizability of research findings: By using standardized tools and instruments, researchers can increase the generalizability of their findings, making it more likely that their results will be applicable to other populations or contexts.</p> <p>Overall, the use of tools and instruments in research for data collection is essential for ensuring the accuracy, reliability, and validity of research findings, and for facilitating comparisons across studies and increasing the generalizability of research results.</p>"},{"location":"local_index/#inclusion-criteria","title":"Inclusion criteria","text":"<ol> <li>the test/tool should focus on L2</li> <li>it should be cited and used in several studies (20 citations)</li> <li>focus on the macro skills (vocabulary, reading, grammar, listening, and speaking, pronunciation) and micro-skills ()</li> <li>used in studies published in the last two decades</li> <li>L2 learners can be children or adult</li> <li>It should be freely accessible</li> <li>Any L2 but mostly English</li> <li>should be published in one of the following journals    a. language learning    b. studies in second language acquisition    c. applied linguistics    d. the modern language    e. TESOL Quarterly</li> </ol>"},{"location":"local_index/#how-to-navigate-the-book","title":"How to navigate the book","text":""},{"location":"local_index/#contributors","title":"Contributors","text":""},{"location":"morphology/","title":"Morphology","text":""},{"location":"morphology/#real-word-decomposition-task","title":"Real Word Decomposition Task","text":"<p>This task was developed by (Carlisle, 2000) to measure learners\u2019 knowledge of morphology. In this task, learners are asked to extract the base form of a word with a derivational suffix (e.g., complexity &gt; complex) to complete a sentence (e.g., the problem was *__*).</p>"},{"location":"morphology/#nonword-derivation-task-nwdt","title":"Nonword derivation task (NWDT)","text":"<p>The task is used to measure morphological awareness (Nagy et al., 2006). Learners are asked to complete a sentence (e.g., this boy is a great__ ) by choosing a nonsense word with appropriate derivational suffix (e.g., tranter) from among four choices (e.g., tranter, tranting, trantion, and traniful) (Kieffer &amp; Lesaux, 2012)</p>"},{"location":"phnology/","title":"Phonology","text":""},{"location":"phnology/#phonological-accuracy","title":"Phonological accuracy","text":""},{"location":"phnology/#abx-discrimination-task","title":"ABX Discrimination task","text":"<p>The ABX Discrimination task is a type of discrimination test used in various fields, including linguistics, psychology, and sensory evaluation. In this task, participants are presented with two samples, A and B, one of which is a control sample, while the other is a modified sample. The participant is then presented with a third sample, X, and is asked to identify which of the two initial samples, A or B, X most closely resembles. This task is commonly used to assess the ability of participants to discriminate between different stimuli and is often used in sensory evaluation research to determine differences in food or beverage products.</p> <p>(Flege, 2003, Gottfried, 1984, Darcy et al, 2016)</p>"},{"location":"pragmatics/","title":"Pragmatics","text":""},{"location":"pragmatics/#a-written-discourse-completion-task","title":"A Written Discourse Completion Task","text":"<pre><code>WDCT is used in pragmatic research to elicit particular speech acts.\n</code></pre>"},{"location":"reading/","title":"Reading","text":""},{"location":"reading/#test-of-silent-word-reading-fluency","title":"Test of Silent Word Reading Fluency","text":"<p>This task measures learners\u2019 ability to how quickly and accurately they can recognize printed words. The test was developed by (Mather, Hammill, Allen, &amp; Roberts, 2004)</p> <p>The Test of Silent Word Reading Fluency (TOSWRF) is a standardized test designed to assess an individual's ability to read words quickly and accurately without vocalizing them. It is intended for use with individuals who are at least eight years old and is particularly useful for assessing reading fluency in individuals with reading difficulties, such as dyslexia.</p> <p>The TOSWRF consists of a list of words that the test taker is asked to read silently as quickly and accurately as possible. The words are presented in a random order, and the test is timed. The test taker's score is based on the number of words they read correctly within a set time limit.</p> <p>The TOSWRF has been shown to have high reliability and validity and can provide valuable information about an individual's reading fluency and potential reading difficulties. It can be used by educators, psychologists, and other professionals to inform instructional planning and interventions.</p>"},{"location":"speech/","title":"Speech","text":""},{"location":"speech/#shadowing-task","title":"Shadowing task","text":"<p>Shadowing is a language learning technique that involves listening to a native speaker or a recorded voice and repeating what is said as quickly and accurately as possible. The goal of shadowing is to improve pronunciation, fluency, and overall speaking ability in a foreign language. The technique involves closely following the speaker's intonation, rhythm, and stress patterns, and attempting to replicate them as closely as possible. Shadowing can be done with a variety of materials, including audio recordings, videos, and live speakers. The technique is often used in language classrooms and language learning programs, and can be done individually or in groups. Shadowing is a highly effective technique for improving speaking skills, as it helps learners to develop their listening comprehension, pronunciation, and overall fluency in a foreign language.</p> <p>The task is used to measure speech segmentation. In this task, participants listen to short excerpts of speech and repeat as many words as they could. (Mitterer &amp; McQueen 2009)</p>"},{"location":"speech/#animacy-judgment","title":"Animacy Judgment","text":"<p>Animacy judgment is a task used in psycholinguistics and cognitive science to investigate how people process and categorize objects based on their perceived animacy. The task involves presenting participants with a series of objects or images and asking them to rate the perceived animacy of each object or image on a scale. The scale typically ranges from inanimate to animate, with intermediate categories such as partially animate or quasi-animate. The task is designed to test the hypothesis that people process animate objects differently from inanimate objects, and that this processing may be influenced by factors such as movement, intentionality, and agency. Animacy judgment tasks have been used in a variety of studies, including studies of language acquisition, semantic processing, and social cognition. The task is often used in conjunction with other measures, such as reaction time or brain imaging, to investigate the neural mechanisms underlying animacy perception and processing.</p> <p>The task is used to assess the speed of lexical access. Participants are presented with some word stimuli and are prompted to decide, as fast as possible, whether words identified either a living (e.g., dog) or nonliving (e.g., chair) item by pressing either the right or left shift key on the computer keyboard. (Segalowitz &amp; Frenkiel-Fishman, 2005).</p>"},{"location":"speech/#sentence-verification-task","title":"Sentence Verification Task","text":"<p>This task measure speed of processing L2 speech. Participants listen to several short sentences and decide if it is true or false. The time needed to repose is used as a measure of processing speed.</p>"},{"location":"speech/#elicited-imitation","title":"Elicited Imitation","text":"<p>This task is considered a reliable and quick assessment of holistic speaking ability and implicit grammar knowledge (Ortega et al., 2002) (Isbell &amp; Son, 2021)</p>"},{"location":"speech/#written-elicited-imitation","title":"Written Elicited Imitation","text":"<pre><code>(Garc\u00eda\u2013Amaya &amp; Cintr\u00f3n\u2013Valent\u00edn, 2021)\n</code></pre>"},{"location":"speech/#picture-drawing-task","title":"Picture drawing task","text":"<p>The picture drawing task is a research method used in psychology, cognitive science, and other fields to investigate visual perception, memory, and creativity. The task involves asking participants to draw a picture based on a verbal or written prompt, such as a word or a short phrase. The prompt can be either concrete or abstract, and can be related to a specific topic or theme. Participants are typically given a set amount of time to complete the drawing, and are encouraged to be as creative as possible.</p> <p>The picture drawing task is often used in studies of visual perception and memory, as it allows researchers to investigate how people represent and remember visual information. The task can also be used to investigate creativity, as it provides a measure of participants' ability to generate novel and original ideas.</p> <p>The picture drawing task can be used in both laboratory and field settings, and can be adapted for use with different populations, including children, adults, and individuals with cognitive or developmental disabilities. The task can be used in conjunction with other methods, such as questionnaires or interviews, to gather additional information about participants' perceptions and experiences.</p> <p>Overall, the picture drawing task is a flexible and versatile research method that can be used to investigate a wide range of topics related to visual perception, memory, and creativity.</p> <p>Participants are asked to draw a picture to illustrate their understanding of reading texts, videos, or other stimuli. The task can help induce top-down meaning-oriented input processing and trigger participants\u2019 meaning-based input processing (Hsieh &amp; Tsai, 2017). It was used in SLA in this study (Lee et al., 2021)</p>"},{"location":"vocabulary/","title":"Vocabulary","text":""},{"location":"vocabulary/#word-and-frequency-lists","title":"Word and Frequency Lists","text":""},{"location":"vocabulary/#the-academic-word-list-awl","title":"The Academic Word List (AWL)","text":"<p>The Academic Word List (AWL) is a list of 570 word families that frequently appear in academic texts, but are not contained in the General Service List (GSL). The AWL was developed by Averil Coxhead at Victoria University of Wellington, New Zealand, with the target readership being English as a second or foreign language students intending to enter English-medium higher education, and teachers of such students. The list is a very useful resource for English for Academic Purposes teachers and learners. The AWL includes lots of resources to practice using the AWL and developing knowledge of academic vocabulary.</p> <p>The Academic Word List (AWL) was developed by Averil Coxhead. The list contains 570 word families which were selected because they appear with great frequency in a broad range of academic texts.</p>"},{"location":"vocabulary/#the-general-service-list","title":"The General Service List","text":"<p>The General Service List (GSL) is a list of approximately 2000 high-frequency words in English that are most commonly used in written and spoken communication. The GSL was developed by Michael West in the 1950s and is widely used in English language teaching and learning materials. The words on the GSL are considered essential for learners of English to know in order to communicate effectively in a variety of contexts. The list includes words such as \"the\", \"and\", \"but\", \"good\", \"bad\", \"happy\", \"sad\", \"big\", \"small\", \"important\", \"interesting\", and many others. The GSL is often used as a basis for vocabulary instruction and assessment in English language classrooms, and is also used by researchers studying language acquisition and corpus linguistics.</p>"},{"location":"vocabulary/#the-new-general-service-list-nsgl","title":"The New General Service List (NSGL)","text":"<p>The New General Service List (NGSL) is a list of the most frequent words in English, which was developed by Dr. Charles Browne, Dr. Brent Culligan, and Dr. Joseph Phillips. The NGSL contains 2800 word families, which make up approximately 92% of all the words in English. The NGSL is an updated version of the General Service List (GSL), which was created in the 1950s by Michael West. The NGSL is based on a large corpus of written and spoken English, and is intended for use in English language teaching and learning. The NGSL is organized into 10 levels, with each level containing approximately 280 word families. The NGSL is a valuable resource for English language learners and teachers, as it provides a solid foundation of the most important vocabulary in English.</p> <p>The New General Service List (NGSL) is published by Dr. Charles Browne, Dr. Brent Culligan and Joseph Phillips and it contains 2,818 words (lemmas) claimed to be the core vocabulary of the English language.</p>"},{"location":"vocabulary/#the-new-academic-word-list-10-nawl","title":"The New Academic Word List 1.0 (NAWL)","text":"<p>The New Academic Word List (NAWL) is a list of 963 word families that frequently appear in academic texts, which was developed by Charles Browne, Brent Culligan, and Joseph Phillips in 2013. The NAWL is an updated version of the Academic Word List (AWL), which was developed by Averil Coxhead in 2000. The NAWL is based on a corpus of academic texts, and is intended for use in English for Academic Purposes (EAP) teaching and learning. The NAWL includes definitions, examples, and collocations for each word family, as well as exercises and activities for learners to practice using the words in context. The NAWL is a valuable resource for EAP teachers and learners, as it provides a targeted list of academic vocabulary that is essential for success in academic settings.</p> <p>The New Academic Word List (NAWL) was developed by Charles Browne, Brent Culligan and Joseph Phillips in 2013. The NAWL is based on a carefully selected academic corpus of 288 million words.</p>"},{"location":"vocabulary/#bnc-frequency-list","title":"BNC Frequency List","text":"<p>The BNC Frequency List is a compilation of the most frequently used words in the British National Corpus (BNC), a large collection of written and spoken texts in British English. The list is designed to aid learners of English as a foreign language and includes vocabulary for foreign travel, study in English, and the internet. Several versions of the frequency list are available, including a lemmatized version and a raw version, and they can be found on various websites such as Kilgarriff's webpage. The list has been used to design graded readers and course books.</p> <p>https://kilgarriff.co.uk/bnc-readme.html https://www.eapfoundation.com/vocab/general/bnccoca/index.php?type=v2</p> <p>Leech, G., &amp; Rayson, P. (2014). Word frequencies in written and spoken English: Based on the British National Corpus. Routledge.</p>"},{"location":"vocabulary/#phrase-list","title":"PHRASE List","text":"<p>The PHRASal Expressions List (PHRASE List) was developed by Martinez and Schmitt. The list conatins the 505 most frequent non-transparent multiword expressions in English, intended especially for receptive use.</p> <p>Martinez, R., &amp; Schmitt, N. (2012). A phrasal expressions list. Applied linguistics, 33(3), 299-320.</p>"},{"location":"vocabulary/#knowledge-based-vocabulary-lists","title":"Knowledge-based Vocabulary Lists","text":"<p>\"The KVL lists were created by testing English language word knowledge of over 100,000 Chinese, German, and Spanish learners of English in order to determine which words were known best and which were less well known. The test required the learners to spell the words correctly, and so required a good level of mastery.\" There are currently three KVL for L2 learners: KVL-Chinese, KVL-German, or KVL-Spanish. The lists are available for download at https://www.britishcouncil.org/exam/aptis/research/knowledge-based-vocabulary-lists-kvl</p>"},{"location":"vocabulary/#vocabulary-knowledge-tests","title":"Vocabulary knowledge Tests","text":""},{"location":"vocabulary/#vocabulary-levels-test-vlt","title":"Vocabulary Levels Test (VLT)","text":"<p>The Vocabulary Levels Test (VLT) is a standardized test used to measure a person's receptive vocabulary, or the number of words they understand when they are presented in context. The test is based on the idea that words can be grouped into levels of difficulty, with each level representing a range of words that are similar in terms of their frequency of use and complexity. The VLT consists of 80 multiple-choice questions, each with a sentence containing a target word and four possible definitions. The test-taker must choose the definition that best fits the context of the sentence. The VLT is often used in research to measure the vocabulary size of individuals and groups, and to compare vocabulary development across different populations. It is also used in language assessment and teaching to determine a learner's level of vocabulary knowledge and to set goals for vocabulary instruction.</p> <p>Validation of this test: Laufer, B. &amp; Nation, P. (1999), A vocabulary size test of controlled productive ability. Language Testing 16(1), 33-51. Rationale for Version C above: Cobb, T. (2000), One size fits all? Canadian Modern Language Review, 57(2), 295-324.</p>"},{"location":"vocabulary/#updated-vocabulary-levels-test-uvlt","title":"Updated Vocabulary Levels Test (UVLT)","text":"<p>The UVLT consists of five levels measuring knowledge of vocabulary at the 1000, 2000, 3000, 4000, and 5000 levels. Items for the tests were sourced from Nation\u2019s (2012) BNC/COCA word lists. The test has two forms ...</p> <pre><code>Webb, S., Sasao, Y., &amp; Ballance, O. (2017). The updated Vocabulary Levels Test: Developing and validating two new forms of the VLT. ITL-International Journal of Applied Linguistics, 168(1), 33-69.\n</code></pre>"},{"location":"vocabulary/#vocabulary-size-test-vst","title":"Vocabulary Size Test (VST)","text":"<p>The Vocabulary Size Test developed by Paul Nation and David Beglar is a widely used tool to estimate the size of an individual's vocabulary in English. The test consists of 140 multiple-choice items, each with four possible answers. The words included in the test range from the most common to the least common, based on frequency data from the British National Corpus.</p> <p>The test is designed to assess receptive vocabulary knowledge, which means that test takers are asked to select the correct meaning of a word from a list of options, rather than producing the word themselves. The test is timed, with a suggested time limit of 50 minutes to complete all 140 items.</p> <p>The Vocabulary Size Test developed by Nation and Beglar has been used in various research studies and has been shown to have high reliability and validity. It has also been used to estimate vocabulary size in different contexts, such as second language learners and academic settings.</p> <pre><code>https://www.lextutor.ca/tests/nation_beglar_size_2007.pdf\n</code></pre>"},{"location":"vocabulary/#computer-adaptive-test-of-size-strength-catss","title":"Computer Adaptive Test of Size &amp; Strength (CATSS)","text":"<p>CATSS aims to assess vocabulary size, i.e. knowledge of word meaning. However, in an attempt to overcome the basic limitation of size tests by testing each word in more detail, four degrees of strength of knowledge are assessed. Each word is tested in four modalities (i.e. productive recall, receptive recall, productive recognition, receptive recognition), as demonstrated using the word \u2018melt\u2019 below.</p> <p>Aviad-Levitzky, T., Laufer, B., &amp; Goldstein, Z. (2019). The new computer adaptive test of size and strength (CATSS): Development and validation. Language Assessment Quarterly, 16(3), 345-368</p>"},{"location":"vocabulary/#picture-vocabulary-size-test-pvst","title":"Picture Vocabulary Size Test (PVST)","text":"<p>The Picture Vocabulary Size Test (PVST) is a test of receptive vocabulary size. The test measures whether the test-taker can find a suitable meaning (a picture) for a given partly contextualized word form. It is a recognition test primarily intended for young pre-literate native speakers up to eight years old and young non-native speakers of English. Two 96-item test sets are included with the test. These test sets use different questions but following the same design procedures. The PVST was designed by Paul Nation of Victoria University of Wellington, New Zealand and implemented as a software package by Laurence Anthony of Waseda University, Japan. Jannie van Hees of The University of Auckland, New Zealand played an important role in trialing the test</p> <pre><code>https://www.lextutor.ca/tests/anthony_nation_2017.pdf\n</code></pre>"},{"location":"vocabulary/#the-peabody-picture-vocabulary-test","title":"The Peabody Picture Vocabulary Test","text":"<p>The Peabody Picture Vocabulary Test (PPVT) is a standardized test that measures receptive vocabulary in children and adults. The test was first published in 1959 by Lloyd M. Dunn and Leota M. Dunn, and has since been revised multiple times. The PPVT assesses an individual's understanding of spoken words by presenting them with a series of pictures and asking them to point to the picture that corresponds with the word they hear. The test is designed to be administered one-on-one and takes approximately 15-20 minutes to complete. The PPVT is commonly used in educational and clinical settings to assess language development and to identify potential language delays or disorders. The test has been translated into multiple languages and is widely used in research on language acquisition and development.</p> <p>(PPVT; Dunn &amp; Dunn, 2007)</p>"},{"location":"vocabulary/#word-part-levels-test-wplt","title":"Word Part Levels Test (WPLT)","text":"<p>The Word Part Levels Test (WPLT) is a vocabulary test developed by Yosuke Sasao and Stuart Webb. It measures learners' knowledge of the meanings and uses of word parts, such as prefixes, suffixes, and roots, in English. WPLT measures three aspects of affix knowledge: form (recognition of written affix forms), meaning (knowledge of affix meanings), and use (knowledge of the syntactic properties of affixes)</p> <p>The WPLT consists of 60 multiple-choice items, each with four possible answers. The test items are designed to assess learners' ability to recognize and use word parts in context. The test is divided into three levels: Level 1 tests knowledge of common prefixes and suffixes, Level 2 tests knowledge of less common word parts and roots, and Level 3 tests knowledge of more complex word parts and their meanings.</p> <p>The WPLT has been shown to have high reliability and validity, and it has been used in various research studies to assess vocabulary knowledge and to guide vocabulary instruction. The test can be used with learners at different levels of proficiency and in different contexts, such as academic and professional settings.</p> <p>Sasao, Y., &amp; Webb, S. (2017). The word part levels test. Language Teaching Research, 21(1), 12-30.</p>"},{"location":"vocabulary/#guessing-from-context-test-gct","title":"Guessing from Context Test (GCT)","text":"<p>The Guessing from Context Test is a language assessment tool that measures a person's ability to understand the meaning of words or phrases based on the context in which they are used. In this test, a person is presented with a sentence or a paragraph that contains a word or phrase that they may not be familiar with. The person is then asked to guess the meaning of the word or phrase based on the context of the sentence or paragraph. This type of test is often used to assess a person's reading comprehension skills and their ability to use context clues to understand unfamiliar words or phrases. The Guessing from Context Test is a useful tool for educators and language professionals to evaluate a person's language proficiency and to identify areas where they may need additional support or instruction.</p> <p>Sasao, Y., &amp; Webb, S. (2018). The guessing from context test. ITL-International Journal of Applied Linguistics, 169(1), 115-141.</p>"},{"location":"vocabulary/#p-lex","title":"P-Lex","text":"<p>(Meara, 2007)</p>"},{"location":"vocabulary/#x_lex","title":"X_Lex","text":"<p>(Meara and Milton, 2003)</p>"},{"location":"vocabulary/#v_capture","title":"V_Capture","text":""},{"location":"vocabulary/#v_size","title":"V_Size","text":""},{"location":"vocabulary/#devlex","title":"DevLex","text":""},{"location":"vocabulary/#totalrecall","title":"TOTALrecall","text":"<p>(Liou et al., 2013)</p>"},{"location":"vocabulary/#webcollocate","title":"WebCollocate","text":"<p>(Chen, 2011)</p>"},{"location":"vocabulary/#webvocabprofile","title":"WebVocabprofile","text":"<p>at www.lextutor.ca</p>"},{"location":"vocabulary/#meaning-recall-task","title":"Meaning recall task","text":"<p>The Meaning Recall Task is a research method used to investigate how people process and remember words and their meanings. In this task, participants are presented with a list of words and asked to recall their meanings. The words can be presented in various ways, such as written or spoken, and can be presented in isolation or in context. After a short delay, participants are asked to write down or say the meanings of the words they remember from the list. This task is often used in studies of vocabulary acquisition and retention, and can help researchers understand how different factors, such as word frequency, context, and personal interest, affect word learning and memory. The Meaning Recall Task can also be adapted to investigate other aspects of word processing, such as word recognition, semantic priming, and word association. Overall, the Meaning Recall Task is a useful tool for investigating the cognitive processes involved in vocabulary learning and use.</p>"},{"location":"vocabulary/#complete-lexical-tutor","title":"Complete Lexical Tutor","text":"<p>The Complete Lexical Tutor is an online tool designed to help users improve their vocabulary knowledge and reading skills. The tool offers a variety of features, including vocabulary quizzes, word frequency lists, and interactive activities that focus on different aspects of vocabulary learning, such as collocations, synonyms, and antonyms. The tool also provides users with feedback on their performance and offers suggestions for further study. In addition, the Complete Lexical Tutor includes a range of resources for teachers and researchers, such as corpus-based vocabulary lists, research articles, and teaching materials. The tool is based on the idea that vocabulary knowledge is a key component of reading comprehension and overall language proficiency, and aims to help users expand their vocabulary and develop their reading skills in a fun and engaging way. The Complete Lexical Tutor is widely used in language teaching and learning contexts, as well as in research on vocabulary acquisition and reading comprehension.</p>"},{"location":"vocabulary/#vocabulary-recognition-test","title":"Vocabulary-Recognition Test","text":"<p>The Vocabulary-Recognition Test is a standardized test used to measure a person's receptive vocabulary, or the number of words they understand when they are presented in isolation. The test is based on the idea that words can be recognized and understood even if they are not used in context. The Vocabulary-Recognition Test consists of a list of words, each presented one at a time, and the test-taker must indicate whether they know the meaning of each word or not. The test is often used in research to measure the vocabulary size of individuals and groups, and to compare vocabulary development across different populations. It is also used in language assessment and teaching to determine a learner's level of vocabulary knowledge and to set goals for vocabulary instruction. However, it should be noted that the Vocabulary-Recognition Test only measures receptive vocabulary and does not assess a person's ability to use words effectively in communication.</p>"},{"location":"vocabulary/#lextale-esp-spanish","title":"Lextale-ESP (Spanish)","text":"<p>A 90-item (60 words +30 nonwords) Spanish vocabulary proficiency test developed by (Education &amp; Advice, 2018) and used in (Cintr\u00f3n-Valent\u00edn &amp; Garci\u00e1-Amaya, 2021)</p>"},{"location":"vocabulary/#phonological-familiarity-judgments","title":"Phonological Familiarity judgments","text":"<p>Phonological familiarity judgments are a research method used to investigate how people perceive and process speech sounds. In this task, participants are presented with pairs of words that differ in their phonological properties, such as vowel length, stress pattern, or consonant voicing. The participants are then asked to judge which of the two words is more familiar or more commonly used in their language. This task is often used in studies of phonological processing and word recognition, and can help researchers understand how different phonological features affect word perception and memory. The task can also be adapted to investigate other aspects of phonological processing, such as phoneme discrimination, phonological awareness, and speech perception in noise. Overall, phonological familiarity judgments are a useful tool for investigating the cognitive processes involved in speech perception and language processing.</p> <p>Orthographic and phonological familiarity judgments The comparative judgment approach (Jones et al., 2019)</p>"},{"location":"vocabulary/#orthographic-familiarity-judgments","title":"Orthographic Familiarity judgments","text":"<p>Orthographic familiarity judgments are a research method used to investigate how people perceive and process written words. In this task, participants are presented with pairs of words that differ in their spelling, such as words with irregular spellings, homophones, or words with different letter combinations. The participants are then asked to judge which of the two words is more familiar or more commonly used in their language. This task is often used in studies of orthographic processing and word recognition, and can help researchers understand how different orthographic features affect word perception and memory. The task can also be adapted to investigate other aspects of orthographic processing, such as phoneme-grapheme correspondence, visual word recognition, and spelling ability. Overall, orthographic familiarity judgments are a useful tool for investigating the cognitive processes involved in reading and written language processing.</p>"},{"location":"vocabulary/#connectives-task","title":"Connectives task","text":"<p>The task assesses learners\u2019 knowledge of connectives (Crosson &amp; Lesaux, 2013). In this untimed cloze task, learners are asked to read a short passage that are missing connectives and choose the connective that appropriately link two sentences or clauses. For example,</p>"},{"location":"vocabulary/#word-associates-test","title":"Word Associates Test","text":"<p>The Word Associates Test is a research method used to investigate the semantic associations between words. In this task, participants are presented with a list of words and asked to generate a list of words that are associated with each of the original words. The associations can be based on any semantic relationship, such as synonyms, antonyms, or related concepts. The Word Associates Test can be used to investigate a variety of research questions related to language and cognition, such as how people organize and retrieve information from memory, how different semantic relationships affect word processing, and how individual differences in semantic knowledge and memory affect language comprehension and production. The task can also be adapted to investigate other aspects of semantic processing, such as semantic priming, semantic similarity, and semantic categorization. Overall, the Word Associates Test is a useful tool for investigating the cognitive and neural mechanisms underlying semantic processing and language comprehension.</p> <p>Read, J. (1993). The development of a new measure of L2 vocabulary knowledge. Language testing, 10(3), 355-371.</p>"},{"location":"vocabulary/#tools-and-software","title":"Tools and Software","text":""},{"location":"vocabulary/#antwordprofiler","title":"AntWordProfiler","text":"<p>AntWordProfiler is a free software tool developed by Dr. Laurence Anthony for analyzing and profiling vocabulary in text. The software is designed to be used with large corpora of text, and can analyze multiple languages. AntWordProfiler provides a range of information about the vocabulary in a text, including frequency, coverage, dispersion, and collocation data. The software can also generate word lists and word clouds, and can identify key clusters of vocabulary in a text. AntWordProfiler is widely used in linguistic research, especially in studies of language acquisition and development, and is also used in language teaching and learning to analyze and profile the vocabulary in textbooks and other materials. The software is available for free download from the AntConc website, and is compatible with Windows, Mac, and Linux operating systems.</p>"},{"location":"vocabulary/#range","title":"RANGE","text":"<p>The Range software is used for analysing the vocabulary load of texts. It can tell you how much and what vocabulary occurs in a particular text or group of texts.</p> <p>https://www.wgtn.ac.nz/lals/resources/paul-nations-resources/vocabulary-analysis-programs</p> <p>(Nation &amp; Heatley, 2002)</p>"},{"location":"vocabulary/#the-lexical-frequency-profile","title":"The Lexical Frequency Profile","text":"<p>The Lexical Frequency Profile (LFP) is a tool used to analyze the frequency of words in a text. It was developed by Michael McCarthy and Ronald Carter in their book \"Language as Discourse: Perspectives for Language Teaching\". The LFP is based on the idea that words in a text can be classified into different frequency bands based on how often they occur in a language. The LFP divides words into four frequency bands: high frequency, mid-high frequency, mid-low frequency, and low frequency. The tool is used to analyze the distribution of words across these frequency bands in a text. The LFP is useful for identifying the most common words in a text, as well as for identifying words that are less common and may require more attention in language teaching and learning. The LFP is widely used in corpus linguistics research, as well as in language teaching and learning to analyze and profile the vocabulary in a text.</p> <p>(Laufer &amp; Nation, 1995)</p>"},{"location":"vocabulary/#the-depth-of-vocabulary-knowledge-test-dvkt","title":"The Depth of Vocabulary Knowledge Test (DVKT)","text":"<p>The Depth of Vocabulary Knowledge Test (DVKT) is a standardized test used to measure a person's depth of vocabulary knowledge, or the extent to which they understand the nuances and subtleties of words beyond their basic definitions. The test is based on the idea that words can be known at different levels of depth, ranging from a basic understanding of their meaning to a more complex understanding of their connotations, associations, and usage in different contexts. The DVKT consists of 40 multiple-choice questions, each with a sentence containing a target word and four possible interpretations. The test-taker must choose the interpretation that best reflects the word's deeper meaning in the given context. The DVKT is often used in research to assess the quality of vocabulary knowledge and to investigate the relationship between vocabulary depth and reading comprehension. It is also used in language teaching and assessment to identify areas of strength and weakness in learners' vocabulary knowledge and to guide instruction in vocabulary development.</p>"}]}